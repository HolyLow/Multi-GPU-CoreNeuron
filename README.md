# Multi-GPU CoreNeuron
> A Multi-GPU version for CoreNeuron

Multi-GPU CoreNeuron is a Multi-GPU version for [CoreNeuron](https://github.com/BlueBrain/CoreNeuron). This version made CoreNeuron be able to run on multi-gpus and GPU cluster,
for the purpose of simulating large scale neural networks.

# Installation

The installation of Multi-GPU CoreNeuron is the same as CoreNeuron. [This tutorial](https://github.com/nrnhines/ringtest) shows how to use CoreNeuron with NEURON.

Before install Multi-GPU CoreNeuron, [NEURON](https://github.com/nrnhines/nrn) and [MOD2C](http://github.com/BlueBrain/mod2c) should be installed at first. 

Below are addtional package dependecies:

* [CMake 2.8.12+](https://cmake.org)
* [PGI OpenACC Compiler >=16.10](https://www.pgroup.com/products/community.htm)
* [MPI 2.0+](http://mpich.org) (The openmpi included in PGI is recommended)
* [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)

You can refer to the installation instructions in [the tutorial](https://github.com/nrnhines/ringtest) to install these dependecies. Please Make sure that mod2c and Multi-GPU CoreNeuron are installed in the same directory (using CMAKE\_INSTALL\_PREFIX). 

After all these dependecies are installed, use the following commands to install Multi-GPU CoreNeuron

```bash
rm -rf build
mkdir build
cd build
module purge
module load /home/pgi/modulefiles/pgi64/16.10 /home/pgi/modulefiles/openmpi/1.10.2/2016
export CC=mpicc
export CXX=mpicxx
cmake .. -DCMAKE_C_FLAGS:STRING="-O2 -lrt -ta=tesla:cuda8.0" -DCMAKE_CXX_FLAGS:STRING="-O2 -lrt -ta=tesla:cuda8.0" -DCOMPILE_LIBRARY_TYPE=STATIC -DCMAKE_INSTALL_PREFIX=/path/to/install -DCUDA_HOST_COMPILER=`which gcc` -DADDITIONAL_MECHPATH=/path/to/addtional/modfiles -DCUDA_PROPAGATE_HOST_FLAGS=OFF -DENABLE_SELECTIVE_GPU_PROFILING=ON -DENABLE_OPENACC=ON
make
make install
```

Some Notes:

#### If you see below error:

```
command not found: module
``` 

You should download [module_tcl], add "eval `tclsh /home/module_tcl/modulecmd.tcl sh autoinit `" in /home/.profile, and source .profile

#### If some errors about CUDA version occur

"-ta=tesla:cuda8.0" means the CUDA version you will use is 8.0, you can modify 8.0 to the CUDA version on your machine, like "-ta=tesla:cuda7.5"

#### If there are some errors like "mod2c is not found"

Maybe it's because the installation directory of mod2c and Multi-GPU CoreNeuron are different, modify -DCMAKE_INSTALL_PREFIX to correct it.

If there are some other errors please refer to [ringtest](https://github.com/nrnhines/ringtest) for more details.

#### Additional Mechanisms

If you have MOD files from the NEURON model, then you have to modify -DADDITIONAL_MECHPATH to your mod directory:
```bash
cmake .. -DADDITIONAL_MECHPATH="/path/of/mod/files/directory/"
```

# Simulation

After installing, you can use the following command to run a test simulation to see whether the installation is correct.
```bash
mpirun -n 1 ./bin/coreneuron_exec -d ../tests/integration/ring -mpi -e 100 --gpu --celsius=6.3 --cell_permute=1
```

This version of CoreNeuron can run on single machine with multi-gpus or GPU cluster. Multiprocess is used to parallel and each process runs on a GPU.

## Simulation on single machine

Before starting simulation on multi-gpus, we should have a network model in parallel generated by [NEURON](https://www.neuron.yale.edu/neuron/). We use [ringtest code](https://github.com/nrnhines/ringtest) as a test. Here is an example:

In /path/of/ringtest, run:
```bash
mpirun -n 4 ./x86_64/special -mpi -python ringtest.py -tstop 100 -coredat coreneuron_data -nring 1024 -ncell 128 -branch 32 64
```
this command run simulation with 4 processes, and generate binary files of parallel network model. Please refer to [ringtest](https://github.com/nrnhines/ringtest) for more details.

Then, in /path/to/multi_gpu_coreneuron/bin, run:
```bash
mpiexec -n 4 ./coreneuron_x86/bin/coreneuron_exec -e 100 -d /path/of/ringtest/coreneuron_data/ -mpi --gpu --celsius=6.3 --cell_permute=1
```
this command will run a simulation on 4 GPUs.

## Simulation on GPU cluster

If you have two GPU servers in a LAN, you can follow [this tutorial](http://mpitutorial.com/tutorials/running-an-mpi-cluster-within-a-lan/) to connect the nodes together. 

Then compile Multi-GPU CoreNeuron in the share directory.

Now, you can run simulation on these two machines:
```bash
mpirun -np 8 --hostfile /home/mpiuser/cloud/mpi_hostfile ./coreneuron_multinode -e 100 -d /path/to/your/model -mpi --gpu --cell_permute=1
```

Please make sure the client machine can access the share directory, otherwise the program will not run successfully.

# Acknowledgements

Many thanks to Joey Wang and Rita Zhang from NVIDIA China for help.
